"use strict";(self.webpackChunkseata_website=self.webpackChunkseata_website||[]).push([[71839],{59886:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>d});var o=n(74848),s=n(28453);const a={title:"Seata New Feature Support -- Undo_Log Compression",author:"chd",keywords:["Seata","undo_log","compress"],date:"2021/05/07"},r="Seata New Feature Support -- Undo_Log Compression",i={permalink:"/seata.github.io/blog/seata-feature-undo-log-compress",editUrl:"https://github.com/apache/incubator-seata-website/blob/docusaurus/i18n/en/docusaurus-plugin-content-blog/seata-feature-undo-log-compress.md",source:"@site/i18n/en/docusaurus-plugin-content-blog/seata-feature-undo-log-compress.md",title:"Seata New Feature Support -- Undo_Log Compression",description:"Current Situation & Pain Points",date:"2021-05-07T00:00:00.000Z",formattedDate:"May 7, 2021",tags:[],readingTime:3.785,hasTruncateMarker:!1,authors:[{name:"chd"}],frontMatter:{title:"Seata New Feature Support -- Undo_Log Compression",author:"chd",keywords:["Seata","undo_log","compress"],date:"2021/05/07"},unlisted:!1,prevItem:{title:"Analysis of Seata's Distributed UUID Generator Based on Improved Snowflake Algorithm",permalink:"/seata.github.io/blog/seata-analysis-UUID-generator"},nextItem:{title:"Seata Deadlock Issue Caused by ConcurrentHashMap",permalink:"/seata.github.io/blog/seata-dsproxy-deadlock"}},l={authorsImageUrls:[void 0]},d=[{value:"Current Situation &amp; Pain Points",id:"current-situation--pain-points",level:2},{value:"Brainstorming",id:"brainstorming",level:2},{value:"Feasibility Analysis",id:"feasibility-analysis",level:2},{value:"Compression Ratio Test:",id:"compression-ratio-test",level:2},{value:"Implementation",id:"implementation",level:3},{value:"Implementation Approach",id:"implementation-approach",level:4},{value:"Partial Code",id:"partial-code",level:4},{value:"peroration",id:"peroration",level:3}];function c(e){const t={code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h2,{id:"current-situation--pain-points",children:"Current Situation & Pain Points"}),"\n",(0,o.jsx)(t.p,{children:"For Seata, it records the before and after data of DML operations to perform possible rollback operations, and stores this data in a blob field in the database. For batch operations such as insert, update, delete, etc., the number of affected rows may be significant, concatenated into a large field inserted into the database, which may lead to the following issues:"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsxs)(t.li,{children:["Exceeding the maximum write limit for a single database operation (such as the ",(0,o.jsx)(t.code,{children:"max_allowed_package"})," parameter in MySQL)."]}),"\n",(0,o.jsx)(t.li,{children:"Significant network IO and database disk IO overhead due to a large amount of data."}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"brainstorming",children:"Brainstorming"}),"\n",(0,o.jsxs)(t.p,{children:["For the first issue, the ",(0,o.jsx)(t.code,{children:"max_allowed_package"}),' parameter limit can be increased based on the actual situation of the business to avoid the "query is too large" problem. For the second issue, increasing bandwidth and using high-performance SSD as the database storage medium can help.']}),"\n",(0,o.jsx)(t.p,{children:"The above solutions involve external or costly measures. Is there a framework-level solution to address the pain points mentioned above?"}),"\n",(0,o.jsx)(t.p,{children:"Considering the root cause of the pain points mentioned above, the problem lies in the generation of excessively large data fields. Therefore, if the corresponding data can be compressed at the business level before data transmission and storage, theoretically, it can solve the problems mentioned above."}),"\n",(0,o.jsx)(t.h2,{id:"feasibility-analysis",children:"Feasibility Analysis"}),"\n",(0,o.jsx)(t.p,{children:"Combining the brainstorming above, in practical development, when large batch operations are required, they are often scheduled during periods of relatively low user activity and low concurrency. At such times, CPU and memory resources can be relatively more utilized to quickly complete the corresponding operations. Therefore, by consuming CPU and memory resources to compress rollback data, the size of data transmission and storage can be reduced."}),"\n",(0,o.jsx)(t.p,{children:"At this point, two things need to be demonstrated:"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsx)(t.li,{children:"After compression, it can reduce the pressure on network IO and database disk IO. This can be measured by the total time taken for data compression + storage in the database."}),"\n",(0,o.jsx)(t.li,{children:"After compression, the efficiency of compression compared to the original data size. This can be measured by the data size before and after compression."}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"Testing the time spent on compressing network usage:"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:"https://user-images.githubusercontent.com/22959373/95567752-f55ddf80-0a55-11eb-8092-1f1d99855bdd.png",alt:"image"})}),"\n",(0,o.jsx)(t.h2,{id:"compression-ratio-test",children:"Compression Ratio Test:"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:"https://user-images.githubusercontent.com/22959373/95567834-0ad30980-0a56-11eb-9d7e-48b74babbea4.png",alt:"image"})}),"\n",(0,o.jsx)(t.p,{children:"The test results clearly indicate that using gzip or zip compression can significantly reduce the pressure on the database and network transmission. At the same time, it can substantially decrease the size of the stored data."}),"\n",(0,o.jsx)(t.h3,{id:"implementation",children:"Implementation"}),"\n",(0,o.jsx)(t.h4,{id:"implementation-approach",children:"Implementation Approach"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:"https://user-images.githubusercontent.com/22959373/116281711-8f039900-a7bc-11eb-91f8-82afdbb9f932.png",alt:"Compression"})}),"\n",(0,o.jsx)(t.h4,{id:"partial-code",children:"Partial Code"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-properties",children:"# Whether to enable undo_log compression, default is true\nseata.client.undo.compress.enable=true\n\n# Compressor type, default is zip, generally recommended to be zip\nseata.client.undo.compress.type=zip\n\n# Compression threshold for enabling compression, default is 64k\nseata.client.undo.compress.threshold=64k\n"})}),"\n",(0,o.jsx)(t.p,{children:"Determining Whether the Undo_Log Compression Feature is Enabled and if the Compression Threshold is Reached"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-java",children:"protected boolean needCompress(byte[] undoLogContent) {\n// 1. Check whether undo_log compression is enabled (1.4.2 Enabled by Default).\n// 2. Check whether the compression threshold has been reached (64k by default).\n// If both return requirements are met, the corresponding undoLogContent is compressed\n    return ROLLBACK_INFO_COMPRESS_ENABLE \n        && undoLogContent.length > ROLLBACK_INFO_COMPRESS_THRESHOLD;\n}\n"})}),"\n",(0,o.jsx)(t.p,{children:"Initiating Compression for Undo_Log After Determining the Need"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-java",children:"// If you need to compress, compress undo_log\nif (needCompress(undoLogContent)) {\n    // Gets the compression type, default zip\n    compressorType = ROLLBACK_INFO_COMPRESS_TYPE;\n    //Get the corresponding compressor and compress it\n    undoLogContent = CompressorFactory.getCompressor(compressorType.getCode()).compress(undoLogContent);\n}\n// else does not need to compress and does not need to do anything\n"})}),"\n",(0,o.jsx)(t.p,{children:"Save the compression type synchronously to the database for use when rolling back:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-java",children:"protected String buildContext(String serializer, CompressorType compressorType) {\n    Map<String, String> map = new HashMap<>();\n    map.put(UndoLogConstants.SERIALIZER_KEY, serializer);\n    // Save the compression type to the database\n    map.put(UndoLogConstants.COMPRESSOR_TYPE_KEY, compressorType.name());\n    return CollectionUtils.encodeMap(map);\n}\n"})}),"\n",(0,o.jsx)(t.p,{children:"Decompress the corresponding information when rolling back:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-java",children:"protected byte[] getRollbackInfo(ResultSet rs) throws SQLException  {\n    // Gets a byte array of rollback information saved to the database\n    byte[] rollbackInfo = rs.getBytes(ClientTableColumnsName.UNDO_LOG_ROLLBACK_INFO);\n    // Gets the compression type\n    // getOrDefault uses the default value CompressorType.NONE to directly upgrade 1.4.2+ to compatible versions earlier than 1.4.2\n    String rollbackInfoContext = rs.getString(ClientTableColumnsName.UNDO_LOG_CONTEXT);\n    Map<String, String> context = CollectionUtils.decodeMap(rollbackInfoContext);\n    CompressorType compressorType = CompressorType.getByName(context.getOrDefault(UndoLogConstants.COMPRESSOR_TYPE_KEY,\n    CompressorType.NONE.name()));\n    // Get the corresponding compressor and uncompress it\n    return CompressorFactory.getCompressor(compressorType.getCode())\n        .decompress(rollbackInfo);\n}\n"})}),"\n",(0,o.jsx)(t.h3,{id:"peroration",children:"peroration"}),"\n",(0,o.jsx)(t.p,{children:"By compressing undo_log, Seata can further improve its performance when processing large amounts of data at the framework level. At the same time, it also provides the corresponding switch and relatively reasonable default value, which is convenient for users to use out of the box, but also convenient for users to adjust according to actual needs, so that the corresponding function is more suitable for the actual use scenario."})]})}function p(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>i});var o=n(96540);const s={},a=o.createContext(s);function r(e){const t=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(a.Provider,{value:t},e.children)}}}]);